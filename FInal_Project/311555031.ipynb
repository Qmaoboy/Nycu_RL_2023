{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behave Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def behavior_policy_gradient(env, behavior_policy, target_policy, num_episodes, max_episode_length, discount_factor, learning_rate):\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        observation = env.reset()\n",
    "\n",
    "        for _ in (t:=tqdm(range(max_episode_length))):\n",
    "            print(observation)\n",
    "            action = behavior_policy.select_action(observation)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            # 計算行為策略的梯度\n",
    "            behavior_prob = behavior_policy.get_probability(observation, action)\n",
    "            target_prob = target_policy.get_probability(observation, action)\n",
    "            gradient = behavior_policy.get_gradient(observation, action)\n",
    "\n",
    "            # 更新行為策略的參數\n",
    "            behavior_policy.update_parameters(gradient, learning_rate)\n",
    "\n",
    "            episode_reward += reward * importance_sampling_ratio\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            observation = next_observation\n",
    "            t.set_description(f\"Action {action},Importance ratio {importance_sampling_ratio}\")\n",
    "            t.set_postfix(f'Episode reward {episode_reward} reward {reward}')\n",
    "            \n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "    average_reward = np.mean(episode_rewards)\n",
    "    average_length = np.mean(episode_lengths)\n",
    "\n",
    "    return average_reward, average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義行為策略和目標策略的類\n",
    "class BehaviorPolicy:\n",
    "    def __init__(self, num_actions, num_features):\n",
    "        self.weights = np.zeros((num_actions, num_features))\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        probabilities = self.get_probabilities(observation)\n",
    "        return np.random.choice(len(probabilities), p=probabilities) ## Select from [2,]\n",
    "\n",
    "    def get_probabilities(self, observation):\n",
    "        print(self.weights,observation)\n",
    "        logits = np.dot(self.weights, observation)\n",
    "        probabilities = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probabilities\n",
    "\n",
    "    def get_probability(self, observation, action):\n",
    "        probabilities = self.get_probabilities(observation)\n",
    "        return probabilities[action]\n",
    "\n",
    "    def get_gradient(self, observation, action):\n",
    "        probabilities = self.get_probabilities(observation)\n",
    "        gradient = np.zeros_like(self.weights)\n",
    "        gradient[action] = observation * (1 - probabilities[action])\n",
    "        return gradient\n",
    "\n",
    "    def update_parameters(self, gradient, learning_rate):\n",
    "        self.weights += learning_rate * gradient\n",
    "\n",
    "\n",
    "class TargetPolicy:\n",
    "    def __init__(self, num_actions, num_features):\n",
    "        self.weights = np.zeros((num_actions, num_features))\n",
    "\n",
    "    def get_probability(self, observation, action):\n",
    "        logits = np.dot(self.weights, observation)\n",
    "        probabilities = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probabilities[action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用範例：\n",
    "env = gym.make('CartPole-v1')  # 使用CartPole-v1環境\n",
    "num_episodes = 1000  # 執行1000個回合\n",
    "max_episode_length = 200  # 每個回合的最大步數\n",
    "discount_factor = 0.99  # 折扣因子\n",
    "learning_rate = 0.01  # 學習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions 2 ,num_features 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04883818, -0.00864507, -0.02790764, -0.01675023], dtype=float32), {})\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] (array([ 0.04883818, -0.00864507, -0.02790764, -0.01675023], dtype=float32), {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,4) and (2,) not aligned: 4 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m behavior_policy \u001b[39m=\u001b[39m BehaviorPolicy(num_actions, num_features)\n\u001b[0;32m      6\u001b[0m target_policy \u001b[39m=\u001b[39m TargetPolicy(num_actions, num_features)\n\u001b[1;32m----> 8\u001b[0m average_reward, average_length \u001b[39m=\u001b[39m behavior_policy_gradient(env, behavior_policy, target_policy, num_episodes, max_episode_length, discount_factor, learning_rate)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage Reward:\u001b[39m\u001b[39m\"\u001b[39m, average_reward)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage Episode Length:\u001b[39m\u001b[39m\"\u001b[39m, average_length)\n",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m, in \u001b[0;36mbehavior_policy_gradient\u001b[1;34m(env, behavior_policy, target_policy, num_episodes, max_episode_length, discount_factor, learning_rate)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m (t\u001b[39m:=\u001b[39mtqdm(\u001b[39mrange\u001b[39m(max_episode_length))):\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(observation)\n\u001b[1;32m---> 23\u001b[0m     action \u001b[39m=\u001b[39m behavior_policy\u001b[39m.\u001b[39;49mselect_action(observation)\n\u001b[0;32m     24\u001b[0m     next_observation, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     25\u001b[0m     \u001b[39m# 計算行為策略的梯度\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mBehaviorPolicy.select_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_action\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m----> 7\u001b[0m     probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_probabilities(observation)\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(probabilities), p\u001b[39m=\u001b[39mprobabilities)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mBehaviorPolicy.get_probabilities\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_probabilities\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights,observation)\n\u001b[1;32m---> 12\u001b[0m     logits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights, observation)\n\u001b[0;32m     13\u001b[0m     probabilities \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(logits) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(logits))\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m probabilities\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,4) and (2,) not aligned: 4 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# 執行Data-Efficient Policy Evaluation with Behavior Policy Gradient\n",
    "num_actions = env.action_space.n\n",
    "num_features = env.observation_space.shape[0]\n",
    "print(f'num_actions {num_actions} ,num_features {num_features}')\n",
    "behavior_policy = BehaviorPolicy(num_actions, num_features)\n",
    "target_policy = TargetPolicy(num_actions, num_features)\n",
    "\n",
    "average_reward, average_length = behavior_policy_gradient(env, behavior_policy, target_policy, num_episodes, max_episode_length, discount_factor, learning_rate)\n",
    "\n",
    "print(\"Average Reward:\", average_reward)\n",
    "print(\"Average Episode Length:\", average_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
